/*
 * Copyright (c) Microsoft. All rights reserved.
 * Licensed under the MIT license. See LICENSE file in the project root for full license information.
 */

package org.apache.spark.streaming.api.csharp

import java.io._
import java.net.{InetAddress, ServerSocket, Socket}
import java.nio.ByteBuffer
import java.util.{ArrayList => JArrayList, HashMap => JHashMap}
import java.util.concurrent.LinkedBlockingDeque

import org.apache.spark.api.csharp.CSharpRDD
import org.apache.spark.api.python.{PythonRDD, PythonRunner, PythonBroadcast}
import org.apache.spark.serializer.SerializerInstance
import org.apache.spark.{TaskContext, Logging, SparkEnv, Partition}
import org.apache.spark.util.RpcUtils
import org.apache.spark.rdd.RDD
import org.apache.spark.broadcast.Broadcast
import org.apache.spark.rpc.{RpcEnv, ThreadSafeRpcEndpoint, RpcEndpointRef}
import org.apache.spark.util.Utils

/**
 Long running task to process one partition data of RDDs generated by CSharpSinkDStream
  */

private [csharp] class SinkProcessor(
    val partitionIndex: Int,
    val command: Array[Byte],
    val envVars: JHashMap[String, String],
    val cSharpIncludes: JArrayList[String],
    val cSharpWorkerExecutable: String,
    val unUsedVersionIdentifier: String,
    val broadcastVars: JArrayList[Broadcast[PythonBroadcast]]) extends Logging {

  // flag to notify C# side
  val RDD_BEGIN = Array[Byte](0x00, 0x00, 0x00, 0x01)
  val RDD_ELEMENT = Array[Byte](0x00, 0x00, 0x00, 0x02)
  val RDD_END = Array[Byte](0x00, 0x00, 0x00, 0x03)

  /* RpcEndpointRef for receiving messages from the SinkProcessorTracker in the driver */
  @volatile private var endpoint: RpcEndpointRef = null

  /* Remote RpcEndpointRef for the SinkProcessorTracker */
  @volatile private var trackerEndpoint: RpcEndpointRef = null

  @volatile private var serializer: SerializerInstance = null
  @volatile private var rddQueue: LinkedBlockingDeque[(RDD[_], Partition)] = null

  private def toBytes(i: Int): Array[Byte] = {
    ByteBuffer.allocate(4).putInt(i).array()
  }

  private def fromBytes(bytes: Array[Byte]): Int = {
    ByteBuffer.wrap(bytes).getInt
  }

  private def initialize(): Unit = {
    endpoint = SparkEnv.get.rpcEnv.setupEndpoint(
      "SinkProcessor-" + partitionIndex + "-" + System.currentTimeMillis(),
      new SinkProcessorEndpoint)
    trackerEndpoint = RpcUtils.makeDriverRef("SinkProcessorTracker",
      SparkEnv.get.conf, SparkEnv.get.rpcEnv)
    serializer = SparkEnv.get.closureSerializer.newInstance()
    rddQueue = new LinkedBlockingDeque[(RDD[_], Partition)]()
  }

  def start(): Unit = {
    initialize()

    val host = SparkEnv.get.blockManager.blockManagerId.host
    val executorId = SparkEnv.get.blockManager.blockManagerId.executorId
    val registerMessage = Tuple5(SinkTrackerMessageType.REGISTER_SINK, partitionIndex, host, executorId, endpoint)
    val Tuple3(_, _, previousAckedRddId) =
      trackerEndpoint.askWithRetry[Tuple3[Int, Int, Int]](registerMessage)
    logInfo(s"receive registerSinkResponse, previousAckedRddId: $previousAckedRddId")

    val cSharpWorker = new File(cSharpWorkerExecutable).getAbsoluteFile
    CSharpRDD.unzip(cSharpWorker.getParentFile)
    val bufferSize = SparkEnv.get.conf.getInt("spark.buffer.size", 65536)
    val reuse_worker = SparkEnv.get.conf.getBoolean("spark.python.worker.reuse", true)
    val taskContext = TaskContext.get()

    val serverSocket = new ServerSocket(0, 1, InetAddress.getByName("localhost"))
    serverSocket.setSoTimeout(60000)
    val serverPort = serverSocket.getLocalPort
    logInfo(s"serverPort: $serverPort")
    val cSharpProcessor = new CSharpProcessor(serverSocket, bufferSize, taskContext)
    cSharpProcessor.start()

    val inputIterator = Iterator(toBytes(previousAckedRddId), toBytes(serverPort))
    envVars.put("SinkProcessor", partitionIndex.toString)
    val runner = new PythonRunner(
      command, envVars, cSharpIncludes, cSharpWorker.getAbsolutePath, unUsedVersionIdentifier,
      broadcastVars, null, bufferSize, reuse_worker)
    val outputIterator = runner.compute(inputIterator, partitionIndex, taskContext)
    for (bytes <- outputIterator) {
      logInfo(s"receive data: ${fromBytes(bytes)} from C# side")
    }
  }

  // Another TCP connection is established between JVM and C# because we need to get full control
  // of the control the flush() operation
  private class CSharpProcessor(
      serverSocket: ServerSocket,
      bufferSize: Int,
      context: TaskContext) extends Thread {
    setDaemon(true)

    private def singleRddIterator(): Iterator[(Array[Byte], Array[Byte])] = {
      val (rdd, part) = rddQueue.take()
      var iter: Iterator[Array[Byte]] = null
      var stop = false
      new Iterator[(Array[Byte], Array[Byte])] {
        override def hasNext = {
          !stop
        }
        override def next() = {
          if (iter == null) {
            iter = rdd.iterator(part, context).asInstanceOf[Iterator[Array[Byte]]]
            logInfo(s"send RDD_BEGIN, rddId: ${rdd.id}")
            (RDD_BEGIN, toBytes(rdd.id))
          } else {
            if (iter.hasNext) {
              (RDD_ELEMENT, iter.next())
            } else {
              logInfo(s"send RDD_END, rddId: ${rdd.id}")
              stop = true
              (RDD_END, toBytes(rdd.id))
            }
          }
        }
      }
    }

    def startReaderThread(worker: Socket): Unit = {
      val readerThread = new Thread() {
        override def run(): Unit = Utils.logUncaughtExceptions {
          try {
            val stream = new BufferedInputStream(worker.getInputStream, bufferSize)
            val dataIn = new DataInputStream(stream)
            while (true) {
              val rddId = dataIn.readInt()
              logInfo(s"receive ack rdd $rddId from C# side")
              trackerEndpoint.send(Tuple3(SinkTrackerMessageType.ACK_RDD, partitionIndex, rddId))
            }
          } catch {
            case e: Exception if context.isCompleted || context.isInterrupted =>
              logDebug("Exception thrown after task completion (likely due to cleanup)", e)
              if (!worker.isClosed) {
                Utils.tryLog(worker.shutdownOutput())
              }
            case e: Exception =>
              // We must avoid throwing exceptions here, because the thread uncaught exception handler
              // will kill the whole executor (see org.apache.spark.executor.Executor).
              if (!worker.isClosed) {
                Utils.tryLog(worker.shutdownOutput())
              }
          }
        }
      }
      readerThread.setDaemon(true)
      readerThread.start()
    }

    override def run(): Unit = Utils.logUncaughtExceptions {
      val worker = serverSocket.accept()
      logInfo(s"accept connection from C# side")
      startReaderThread(worker)
      try {
        val stream = new BufferedOutputStream(worker.getOutputStream, bufferSize)
        val dataOut = new DataOutputStream(stream)
        while (true) {
          val rddIter = singleRddIterator()
          PythonRDD.writeIteratorToStream(rddIter, dataOut)
          dataOut.flush()
        }
      } catch {
        case e: Exception if context.isCompleted || context.isInterrupted =>
          logDebug("Exception thrown after task completion (likely due to cleanup)", e)
          if (!worker.isClosed) {
            Utils.tryLog(worker.shutdownOutput())
          }
        case e: Exception =>
          // We must avoid throwing exceptions here, because the thread uncaught exception handler
          // will kill the whole executor (see org.apache.spark.executor.Executor).
          if (!worker.isClosed) {
            Utils.tryLog(worker.shutdownOutput())
          }
      }
    }
  }

  private class SinkProcessorEndpoint() extends ThreadSafeRpcEndpoint {
    override val rpcEnv: RpcEnv = SparkEnv.get.rpcEnv

    override def receive: PartialFunction[Any, Unit] = {
      case Tuple2(
        SinkTrackerMessageType.ADD_RDD_PARTITIONS,
        rddAndPartitions: List[Tuple2[Partition, Broadcast[Array[Byte]]]]) =>
        logInfo("Received AddRddAndPartitions signal from tracker")
        rddAndPartitions.foreach( {
          case Tuple2(partition, rddBinary) =>
            val rdd = serializer.deserialize[RDD[_]](ByteBuffer.wrap(rddBinary.value))
            logInfo(s"add rdd[${rdd.id}] to queue")
            rddQueue.add(rdd, partition)
        })
    }
  }

}
